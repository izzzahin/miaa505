{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "be808d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "89cd2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9ceb77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available()else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ff047d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7f4aebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4c59fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset root directory\n",
    "data_dir = 'dataset_flower102/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ed9b0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "train_dataset = torchvision.datasets.Flowers102(root= data_dir, split = 'train', transform = data_transforms[\"train\"], download = True)\n",
    "\n",
    "#load test data\n",
    "test_dataset = torchvision.datasets.Flowers102(root= data_dir, split = 'test', transform = data_transforms[\"test\"], download = True)\n",
    "\n",
    "#load valid\n",
    "valid_dataset = torchvision.datasets.Flowers102(root= data_dir, split = 'val', transform = data_transforms[\"val\"], download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "845bdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b02c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "951b0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement conv net\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = (3,3), stride = (1,1), padding = (1,1))\n",
    "        \n",
    "        #pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n",
    "        \n",
    "        #second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels=32, kernel_size=(3,3), stride = (1,1), padding = (1,1))\n",
    "        \n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        #convolution + RelU + Pooling layer 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        #convolutional + ReLU + Pooling layer 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #flatten the tensor for fully connected layer\n",
    "        #x = x.view(-1, 32 * 56 * 56)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        #fully connected layer 1 with ReLU and Dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "23813890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "653e131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ea907043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step[8/32], Loss: 5.8847\n",
      "Epoch [1/50], Step[16/32], Loss: 4.6363\n",
      "Epoch [1/50], Step[24/32], Loss: 4.6059\n",
      "Epoch [1/50], Step[32/32], Loss: 4.5471\n",
      "Epoch [2/50], Step[8/32], Loss: 4.6163\n",
      "Epoch [2/50], Step[16/32], Loss: 4.5721\n",
      "Epoch [2/50], Step[24/32], Loss: 4.5494\n",
      "Epoch [2/50], Step[32/32], Loss: 4.4520\n",
      "Epoch [3/50], Step[8/32], Loss: 4.3033\n",
      "Epoch [3/50], Step[16/32], Loss: 4.5609\n",
      "Epoch [3/50], Step[24/32], Loss: 4.5950\n",
      "Epoch [3/50], Step[32/32], Loss: 4.5842\n",
      "Epoch [4/50], Step[8/32], Loss: 4.4051\n",
      "Epoch [4/50], Step[16/32], Loss: 4.5180\n",
      "Epoch [4/50], Step[24/32], Loss: 4.0175\n",
      "Epoch [4/50], Step[32/32], Loss: 4.3948\n",
      "Epoch [5/50], Step[8/32], Loss: 4.3085\n",
      "Epoch [5/50], Step[16/32], Loss: 4.2646\n",
      "Epoch [5/50], Step[24/32], Loss: 4.2753\n",
      "Epoch [5/50], Step[32/32], Loss: 4.2828\n",
      "Epoch [6/50], Step[8/32], Loss: 3.8649\n",
      "Epoch [6/50], Step[16/32], Loss: 4.1342\n",
      "Epoch [6/50], Step[24/32], Loss: 4.1774\n",
      "Epoch [6/50], Step[32/32], Loss: 4.2486\n",
      "Epoch [7/50], Step[8/32], Loss: 3.8819\n",
      "Epoch [7/50], Step[16/32], Loss: 4.0749\n",
      "Epoch [7/50], Step[24/32], Loss: 3.9205\n",
      "Epoch [7/50], Step[32/32], Loss: 4.0337\n",
      "Epoch [8/50], Step[8/32], Loss: 3.2622\n",
      "Epoch [8/50], Step[16/32], Loss: 3.5318\n",
      "Epoch [8/50], Step[24/32], Loss: 3.7416\n",
      "Epoch [8/50], Step[32/32], Loss: 3.8133\n",
      "Epoch [9/50], Step[8/32], Loss: 3.5059\n",
      "Epoch [9/50], Step[16/32], Loss: 2.9655\n",
      "Epoch [9/50], Step[24/32], Loss: 3.2556\n",
      "Epoch [9/50], Step[32/32], Loss: 3.1689\n",
      "Epoch [10/50], Step[8/32], Loss: 2.9683\n",
      "Epoch [10/50], Step[16/32], Loss: 2.5734\n",
      "Epoch [10/50], Step[24/32], Loss: 2.9894\n",
      "Epoch [10/50], Step[32/32], Loss: 4.0333\n",
      "Epoch [11/50], Step[8/32], Loss: 2.1754\n",
      "Epoch [11/50], Step[16/32], Loss: 1.9691\n",
      "Epoch [11/50], Step[24/32], Loss: 2.7189\n",
      "Epoch [11/50], Step[32/32], Loss: 2.8335\n",
      "Epoch [12/50], Step[8/32], Loss: 1.7842\n",
      "Epoch [12/50], Step[16/32], Loss: 2.1735\n",
      "Epoch [12/50], Step[24/32], Loss: 1.8530\n",
      "Epoch [12/50], Step[32/32], Loss: 2.3440\n",
      "Epoch [13/50], Step[8/32], Loss: 2.1156\n",
      "Epoch [13/50], Step[16/32], Loss: 1.8837\n",
      "Epoch [13/50], Step[24/32], Loss: 2.0003\n",
      "Epoch [13/50], Step[32/32], Loss: 2.1827\n",
      "Epoch [14/50], Step[8/32], Loss: 1.6457\n",
      "Epoch [14/50], Step[16/32], Loss: 1.6085\n",
      "Epoch [14/50], Step[24/32], Loss: 1.5211\n",
      "Epoch [14/50], Step[32/32], Loss: 1.8940\n",
      "Epoch [15/50], Step[8/32], Loss: 1.1453\n",
      "Epoch [15/50], Step[16/32], Loss: 1.6321\n",
      "Epoch [15/50], Step[24/32], Loss: 1.2674\n",
      "Epoch [15/50], Step[32/32], Loss: 1.7975\n",
      "Epoch [16/50], Step[8/32], Loss: 0.9449\n",
      "Epoch [16/50], Step[16/32], Loss: 1.1983\n",
      "Epoch [16/50], Step[24/32], Loss: 1.2389\n",
      "Epoch [16/50], Step[32/32], Loss: 1.2941\n",
      "Epoch [17/50], Step[8/32], Loss: 1.3956\n",
      "Epoch [17/50], Step[16/32], Loss: 0.7311\n",
      "Epoch [17/50], Step[24/32], Loss: 0.9850\n",
      "Epoch [17/50], Step[32/32], Loss: 0.7222\n",
      "Epoch [18/50], Step[8/32], Loss: 0.9405\n",
      "Epoch [18/50], Step[16/32], Loss: 1.0322\n",
      "Epoch [18/50], Step[24/32], Loss: 1.0151\n",
      "Epoch [18/50], Step[32/32], Loss: 1.1223\n",
      "Epoch [19/50], Step[8/32], Loss: 1.1250\n",
      "Epoch [19/50], Step[16/32], Loss: 0.8937\n",
      "Epoch [19/50], Step[24/32], Loss: 0.6943\n",
      "Epoch [19/50], Step[32/32], Loss: 0.5064\n",
      "Epoch [20/50], Step[8/32], Loss: 1.4571\n",
      "Epoch [20/50], Step[16/32], Loss: 1.2260\n",
      "Epoch [20/50], Step[24/32], Loss: 0.6969\n",
      "Epoch [20/50], Step[32/32], Loss: 0.6621\n",
      "Epoch [21/50], Step[8/32], Loss: 0.6497\n",
      "Epoch [21/50], Step[16/32], Loss: 0.8205\n",
      "Epoch [21/50], Step[24/32], Loss: 0.5477\n",
      "Epoch [21/50], Step[32/32], Loss: 0.6416\n",
      "Epoch [22/50], Step[8/32], Loss: 0.7429\n",
      "Epoch [22/50], Step[16/32], Loss: 0.9908\n",
      "Epoch [22/50], Step[24/32], Loss: 0.9771\n",
      "Epoch [22/50], Step[32/32], Loss: 0.8418\n",
      "Epoch [23/50], Step[8/32], Loss: 0.4056\n",
      "Epoch [23/50], Step[16/32], Loss: 0.9613\n",
      "Epoch [23/50], Step[24/32], Loss: 0.3996\n",
      "Epoch [23/50], Step[32/32], Loss: 0.6461\n",
      "Epoch [24/50], Step[8/32], Loss: 0.9446\n",
      "Epoch [24/50], Step[16/32], Loss: 0.3507\n",
      "Epoch [24/50], Step[24/32], Loss: 0.7215\n",
      "Epoch [24/50], Step[32/32], Loss: 0.8297\n",
      "Epoch [25/50], Step[8/32], Loss: 0.5581\n",
      "Epoch [25/50], Step[16/32], Loss: 0.5782\n",
      "Epoch [25/50], Step[24/32], Loss: 0.5051\n",
      "Epoch [25/50], Step[32/32], Loss: 0.4570\n",
      "Epoch [26/50], Step[8/32], Loss: 0.5808\n",
      "Epoch [26/50], Step[16/32], Loss: 0.5514\n",
      "Epoch [26/50], Step[24/32], Loss: 0.4199\n",
      "Epoch [26/50], Step[32/32], Loss: 0.5897\n",
      "Epoch [27/50], Step[8/32], Loss: 0.5431\n",
      "Epoch [27/50], Step[16/32], Loss: 0.4048\n",
      "Epoch [27/50], Step[24/32], Loss: 0.4133\n",
      "Epoch [27/50], Step[32/32], Loss: 0.5798\n",
      "Epoch [28/50], Step[8/32], Loss: 0.5865\n",
      "Epoch [28/50], Step[16/32], Loss: 0.4844\n",
      "Epoch [28/50], Step[24/32], Loss: 0.5464\n",
      "Epoch [28/50], Step[32/32], Loss: 0.4835\n",
      "Epoch [29/50], Step[8/32], Loss: 0.4292\n",
      "Epoch [29/50], Step[16/32], Loss: 0.5151\n",
      "Epoch [29/50], Step[24/32], Loss: 0.9727\n",
      "Epoch [29/50], Step[32/32], Loss: 0.6600\n",
      "Epoch [30/50], Step[8/32], Loss: 0.5553\n",
      "Epoch [30/50], Step[16/32], Loss: 0.5735\n",
      "Epoch [30/50], Step[24/32], Loss: 0.5414\n",
      "Epoch [30/50], Step[32/32], Loss: 0.8991\n",
      "Epoch [31/50], Step[8/32], Loss: 0.5095\n",
      "Epoch [31/50], Step[16/32], Loss: 0.8006\n",
      "Epoch [31/50], Step[24/32], Loss: 0.8553\n",
      "Epoch [31/50], Step[32/32], Loss: 0.3939\n",
      "Epoch [32/50], Step[8/32], Loss: 0.5523\n",
      "Epoch [32/50], Step[16/32], Loss: 0.5342\n",
      "Epoch [32/50], Step[24/32], Loss: 0.3575\n",
      "Epoch [32/50], Step[32/32], Loss: 0.2604\n",
      "Epoch [33/50], Step[8/32], Loss: 0.3749\n",
      "Epoch [33/50], Step[16/32], Loss: 0.3121\n",
      "Epoch [33/50], Step[24/32], Loss: 0.6963\n",
      "Epoch [33/50], Step[32/32], Loss: 0.5497\n",
      "Epoch [34/50], Step[8/32], Loss: 0.3525\n",
      "Epoch [34/50], Step[16/32], Loss: 0.3179\n",
      "Epoch [34/50], Step[24/32], Loss: 0.4385\n",
      "Epoch [34/50], Step[32/32], Loss: 0.4841\n",
      "Epoch [35/50], Step[8/32], Loss: 0.4128\n",
      "Epoch [35/50], Step[16/32], Loss: 0.7325\n",
      "Epoch [35/50], Step[24/32], Loss: 0.8752\n",
      "Epoch [35/50], Step[32/32], Loss: 0.2107\n",
      "Epoch [36/50], Step[8/32], Loss: 0.6933\n",
      "Epoch [36/50], Step[16/32], Loss: 0.7346\n",
      "Epoch [36/50], Step[24/32], Loss: 0.5369\n",
      "Epoch [36/50], Step[32/32], Loss: 0.5517\n",
      "Epoch [37/50], Step[8/32], Loss: 0.5650\n",
      "Epoch [37/50], Step[16/32], Loss: 0.3591\n",
      "Epoch [37/50], Step[24/32], Loss: 0.3388\n",
      "Epoch [37/50], Step[32/32], Loss: 0.5815\n",
      "Epoch [38/50], Step[8/32], Loss: 0.6048\n",
      "Epoch [38/50], Step[16/32], Loss: 0.5596\n",
      "Epoch [38/50], Step[24/32], Loss: 0.2485\n",
      "Epoch [38/50], Step[32/32], Loss: 0.3868\n",
      "Epoch [39/50], Step[8/32], Loss: 0.1412\n",
      "Epoch [39/50], Step[16/32], Loss: 0.4846\n",
      "Epoch [39/50], Step[24/32], Loss: 0.5895\n",
      "Epoch [39/50], Step[32/32], Loss: 0.1610\n",
      "Epoch [40/50], Step[8/32], Loss: 0.2630\n",
      "Epoch [40/50], Step[16/32], Loss: 0.5435\n",
      "Epoch [40/50], Step[24/32], Loss: 0.2142\n",
      "Epoch [40/50], Step[32/32], Loss: 0.2532\n",
      "Epoch [41/50], Step[8/32], Loss: 0.4398\n",
      "Epoch [41/50], Step[16/32], Loss: 0.2689\n",
      "Epoch [41/50], Step[24/32], Loss: 0.3773\n",
      "Epoch [41/50], Step[32/32], Loss: 0.3998\n",
      "Epoch [42/50], Step[8/32], Loss: 0.5114\n",
      "Epoch [42/50], Step[16/32], Loss: 0.1152\n",
      "Epoch [42/50], Step[24/32], Loss: 0.2375\n",
      "Epoch [42/50], Step[32/32], Loss: 0.4760\n",
      "Epoch [43/50], Step[8/32], Loss: 0.6281\n",
      "Epoch [43/50], Step[16/32], Loss: 0.4693\n",
      "Epoch [43/50], Step[24/32], Loss: 0.2757\n",
      "Epoch [43/50], Step[32/32], Loss: 0.4305\n",
      "Epoch [44/50], Step[8/32], Loss: 0.3935\n",
      "Epoch [44/50], Step[16/32], Loss: 0.3480\n",
      "Epoch [44/50], Step[24/32], Loss: 0.1150\n",
      "Epoch [44/50], Step[32/32], Loss: 0.5876\n",
      "Epoch [45/50], Step[8/32], Loss: 0.3477\n",
      "Epoch [45/50], Step[16/32], Loss: 0.4162\n",
      "Epoch [45/50], Step[24/32], Loss: 0.6533\n",
      "Epoch [45/50], Step[32/32], Loss: 0.2701\n",
      "Epoch [46/50], Step[8/32], Loss: 0.2977\n",
      "Epoch [46/50], Step[16/32], Loss: 0.2340\n",
      "Epoch [46/50], Step[24/32], Loss: 0.2457\n",
      "Epoch [46/50], Step[32/32], Loss: 0.5812\n",
      "Epoch [47/50], Step[8/32], Loss: 0.3192\n",
      "Epoch [47/50], Step[16/32], Loss: 0.3466\n",
      "Epoch [47/50], Step[24/32], Loss: 0.4192\n",
      "Epoch [47/50], Step[32/32], Loss: 0.6295\n",
      "Epoch [48/50], Step[8/32], Loss: 0.3271\n",
      "Epoch [48/50], Step[16/32], Loss: 0.4935\n",
      "Epoch [48/50], Step[24/32], Loss: 0.4865\n",
      "Epoch [48/50], Step[32/32], Loss: 0.2167\n",
      "Epoch [49/50], Step[8/32], Loss: 0.2576\n",
      "Epoch [49/50], Step[16/32], Loss: 0.1561\n",
      "Epoch [49/50], Step[24/32], Loss: 0.1433\n",
      "Epoch [49/50], Step[32/32], Loss: 0.5914\n",
      "Epoch [50/50], Step[8/32], Loss: 0.1200\n",
      "Epoch [50/50], Step[16/32], Loss: 0.3840\n",
      "Epoch [50/50], Step[24/32], Loss: 0.3013\n",
      "Epoch [50/50], Step[32/32], Loss: 0.7930\n"
     ]
    }
   ],
   "source": [
    "n_steps_total = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        #forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 8 == 0:\n",
    "            print(f\"Epoch [{epoch+ 1}/{num_epochs}], Step[{i+1}/{n_steps_total}], Loss: {loss.item():.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d362a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26cc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "eaa61af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"flowers-102.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "610fa718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize correct predictions,valid loss and total samples counter to 0\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in valid_loader:\n",
    "            #move data and targets to the appropriate device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "        \n",
    "            #Forward pass: compute predfictions\n",
    "            scores = model(data)\n",
    "        \n",
    "            #get the predicted class with the highest score\n",
    "            _, predicted = torch.max(scores, 1)\n",
    "        \n",
    "            #update the count of correct prediction\n",
    "            n_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "            #update the count of total samples\n",
    "            n_samples += targets.size(0) \n",
    "            \n",
    "    accuracy = 100.0 *n_correct / n_samples\n",
    "    print(f'Got {n_correct} / {n_samples} with accuracy   {accuracy:.2f}%')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31015365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c8aa6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 170 / 1020 with accuracy   16.67%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c93ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
